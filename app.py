# -*- coding: utf-8 -*-
import gradio as gr
import pandas as pd
from dotenv import load_dotenv
import os

#IBM_Cloud##########################################################################
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods
from ibm_watsonx_ai import Credentials
from ibm_watsonx_ai.foundation_models import ModelInference

Watsonx_ai_url = 'https://us-south.ml.cloud.ibm.com'

load_dotenv()
IBM_Cloud_API = os.getenv('api_key')
IBM_Project_ID = os.getenv('project_id')


def IBM_LLMS(ibm_model,systemp,DATA):
	# ibm_model = 'ibm/granite-3-8b-instruct'
	ibm_models=['ibm/granite-3-8b-instruct','ibm/granite-3-2-8b-instruct-preview-rc','meta-llama/llama-3-1-8b-instruct']
	Pindex=0 if ibm_models.index(ibm_model)<=1 else ibm_models.index(ibm_model)-1
	credentials = Credentials(
    	url=Watsonx_ai_url,
    	api_key=IBM_Cloud_API,
	)
	parameters = {
		GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,
    	GenParams.MAX_NEW_TOKENS: 1000,
    	GenParams.MIN_NEW_TOKENS: 1,
    	GenParams.TEMPERATURE: 0.5,
    	GenParams.TOP_K: 50,
    	GenParams.TOP_P: 1
	}
	model = ModelInference(
    	model_id = ibm_model,
    	params = parameters,
    	credentials = credentials,
    	project_id = IBM_Project_ID,
    )
# ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
	prompt_inputs = [f"""
<|start_of_role|>system<|end_of_role|>
Knowledge Cutoff Date: April 2024.
Today's Date: December 16, 2024.
You are Granite, developed by IBM. You are a helpful AI assistant with access to the following tools. When a tool is required to answer the user's query, respond with <|tool_call|> followed by a JSON list of tools used. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request.
Answer questions briefly and do not provide unnecessary explanation.
You are a Korean model, SO ANSWER IN KOREAN.{systemp}.Don't fill in the format.
User question:{DATA}<|end_of_text|>""",
f"""<|start_header_id|>system<|end_header_id|>
You always answer the questions with markdown formatting using GitHub syntax. 
The markdown formatting you support: headings, bold, italic, links, tables, lists, code blocks, and blockquotes. 
You must omit that you answer the questions with markdown.
Any HTML tags must be wrapped in block quotes, for example ```<html>```. 
You will be penalized for not rendering code in block quotes.
When returning code blocks, specify language.
You are a helpful, respectful and honest assistant. 
Always answer as helpfully as possible, while being safe.
Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. 
Please ensure that your responses are socially unbiased and positive in nature.
If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. 
If you don't know the answer to a question, please don't share false information.<|eot_id|><|start_header_id|>user<|end_header_id|>
Only answer questions concisely.
You are a Korean model, SO ANSWER IN KOREAN.{systemp}.Don't fill in the format.
User question:{DATA}<|end_of_text|>"""]

	generated_response = model.generate_text(prompt=prompt_inputs[Pindex], guardrails=False)
 
	return (f"{generated_response}")


#Google Gemini######################################################################
import google.generativeai as genai
Google_Gemini_API = os.getenv('google_api_key')

def GOOGLE_LLM(systemp, DATA):
	SYSTEM_PROMPT = systemp
	genai.configure(api_key=Google_Gemini_API)
	model = genai.GenerativeModel('gemini-1.5-flash')
	response = model.generate_content(SYSTEM_PROMPT + DATA)
	res = {response.text}

	return list(res)[0].strip('\n?')

#Google Gemini######################################################################

####################################
# (A) ê°€ìƒ ëª¨ë¸ ì‘ë‹µ
####################################

def simulate_model_responses(systemp, question, model_list):
    """
    ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œ ëŒ€ì‹  ê°„ë‹¨ížˆ ê°€ìƒì˜ ì‘ë‹µì„ ë§Œë“­ë‹ˆë‹¤.
    """
    responses = {}
    real_model = ['gemini-1.5-flash', 'ibm/granite-3-8b-instruct','ibm/granite-3-2-8b-instruct-preview-rc','meta-llama/llama-3-1-8b-instruct']
    
            
    # for m in model_list:
    #     responses[m] = f"{m}ì˜ ë‹µë³€: '{question}' ì— ëŒ€í•œ ê°€ìƒ ì‘ë‹µ."
    #     responses[m] = GOOGLE_LLM(systemp, question)
    #     responses[m] = IBM_LLMS(systemp, question)
    cnt = 0
    for m in model_list:
        if cnt == 0:
            responses[m] = GOOGLE_LLM(systemp, question)
        else:
            responses[m] = IBM_LLMS(real_model[cnt], systemp, question)
        cnt += 1
    
    return responses

####################################
# (B) ì—…/ë‹¤ìš´ í† ê¸€
####################################
def toggle_vote(vote_state, model):
    """
    - vote_state = { "Model_A": "down"/"up", ... }
    - í´ë¦­ ì‹œ í˜„ìž¬ ìƒíƒœë¥¼ í† ê¸€, ë²„íŠ¼ ë¼ë²¨ ë°˜ì˜
    """
    current = vote_state.get(model, "down")
    new_val = "up" if current == "down" else "down"
    vote_state[model] = new_val

    emoji = "ðŸ‘" if new_val == "up" else "ðŸ‘Ž"
    label = f"{model} ({emoji})"
    return vote_state, label

####################################
# (C) ì§ˆë¬¸ ë³´ë‚´ê¸°
####################################


def submit_question(systemp, question, active_models, vote_state):
    if not question.strip():
        return ("ì§ˆë¬¸ì´ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤.",)*4 + (vote_state,)

    responses = simulate_model_responses(systemp, question, active_models)
    rA = responses.get("Model_A", "Model_AëŠ” ì œì™¸ë¨.")
    rB = responses.get("Model_B", "Model_BëŠ” ì œì™¸ë¨.")
    rC = responses.get("Model_C", "Model_CëŠ” ì œì™¸ë¨.")
    rD = responses.get("Model_D", "Model_DëŠ” ì œì™¸ë¨.")

    return rA, rB, rC, rD, vote_state

####################################
# (D) ë¼ìš´ë“œ ì§„í–‰ (ìžë™ í™•ì •)
####################################
def next_round_and_auto_finalize(vote_state, active_models):
    model_match = {"Model_A":'gemini-1.5-flash', "Model_B":'ibm/granite-3-8b-instruct',"Model_C":'ibm/granite-3-2-8b-instruct-preview-rc',"Model_D":'meta-llama/llama-3-1-8b-instruct'}
    up_models = [m for m in active_models if vote_state.get(m,"down")=="up"]
    auto_final = False
    final_msg = ""
    final_series = pd.Series([], dtype=object)

    if len(up_models)==0:
        round_msg = "í˜„ìž¬ ë¼ìš´ë“œì—ì„œ ì—…(ðŸ‘)ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë‘ íƒˆë½."
        new_models = []
    elif len(up_models)==1:
        only_m = up_models[0]
        round_msg = f"'{only_m}' í•œ ê°œë§Œ ì—…(ðŸ‘) => ìžë™ ìµœì¢… í™•ì •!"
        final_msg = f"ìµœì¢… ëª¨ë¸ì€ '{model_match[only_m]}'ìž…ë‹ˆë‹¤!"
        final_series = pd.Series([only_m])
        auto_final = True
        new_models = [only_m]
    else:
        round_msg = f"ì—…(ðŸ‘)ëœ ëª¨ë¸: {up_models}"
        new_models = up_models

    hideA = gr.update(visible=("Model_A" in new_models))
    hideB = gr.update(visible=("Model_B" in new_models))
    hideC = gr.update(visible=("Model_C" in new_models))
    hideD = gr.update(visible=("Model_D" in new_models))

    show_restart = gr.update(visible=auto_final)

    return (
        round_msg,
        new_models,
        hideA, hideB, hideC, hideD,
        final_msg,
        final_series,
        auto_final,
        show_restart
    )

####################################
# (E) ë¦¬ë”ë³´ë“œ ê°±ì‹ 
####################################
def update_score(score_dict, final_series):
    """
    final_series ë‚´ ëª¨ë¸ë“¤ => +1ì 
    """
    if not final_series.empty:
        for m in final_series:
            score_dict[m] = score_dict.get(m, 0) + 1

    df = pd.DataFrame(list(score_dict.items()), columns=["Model","Score"])
    df.sort_values(by="Score", ascending=False, inplace=True)
    return score_dict, df

def finalize_models_score(vote_state, active_models, score_dict):
    model_match = {"Model_A":'gemini-1.5-flash', "Model_B":'ibm/granite-3-8b-instruct',"Model_C":'ibm/granite-3-2-8b-instruct-preview-rc',"Model_D":'meta-llama/llama-3-1-8b-instruct'}
    """
    ì—…ëœ ëª¨ë¸ë“¤ => ìµœì¢… í™•ì • => ì ìˆ˜ ë°˜ì˜
    """
    ups = [m for m in active_models if vote_state.get(m,"down")=="up"]
    final_series = pd.Series(ups, dtype=object)

    if len(ups) == 0:
        msg = "ì—…ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìµœì¢…ì„ íƒ ë¶ˆê°€."
    elif len(ups) == 1:
        msg = f"ìµœì¢… ëª¨ë¸ì€ '{model_match[ups[0]]}'ìž…ë‹ˆë‹¤!"
    else:
        msg = f"ìµœì¢… ëª¨ë¸ì´ ì—¬ëŸ¬ ê°œìž…ë‹ˆë‹¤: {ups}"

    auto_final = (len(ups) >= 1)
    show_restart = gr.update(visible=auto_final)

    # ì ìˆ˜ ì—…ë°ì´íŠ¸
    new_score, new_df = update_score(score_dict, final_series)

    return msg, final_series, auto_final, show_restart, new_score, new_df

####################################
# (F) ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œìž‘ (ì ìˆ˜ ìœ ì§€)
####################################
def restart_all_but_keep_score(active_models, vote_state, final_series):
    """íˆ¬í‘œ ìƒíƒœë§Œ ì´ˆê¸°í™”, ì ìˆ˜ ê·¸ëŒ€ë¡œ"""
    init_models = ["Model_A","Model_B","Model_C","Model_D"]
    new_vote = {m:"down" for m in init_models}
    new_series = pd.Series([], dtype=object)

    # ëª¨ë¸ ì—´ ëª¨ë‘ visible=True ë³µêµ¬
    showA= gr.update(visible=True)
    showB= gr.update(visible=True)
    showC= gr.update(visible=True)
    showD= gr.update(visible=True)
    showE= gr.update(visible=True)
    showF= gr.update(visible=True)

    round_msg = "ìƒˆë¡œ ì‹œìž‘í•©ë‹ˆë‹¤. ì§ˆë¬¸ ìž…ë ¥ í›„ ì§„í–‰í•˜ì„¸ìš”."
    final_msg = ""
    hide_restart = gr.update(visible=False)

    return (
        init_models, new_vote, new_series,    # active_models, vote_state, final_series
        showA, showB, showC, showD, showE, showF,
        round_msg, final_msg,
        False,     # auto_finalized
        hide_restart
    )

####################################
# (G) ë©”ì¸ App (Tabs: Vote / Leaderboard)
####################################
def build_app():
    with gr.Blocks() as demo:
        with gr.Tabs():
            # Vote íƒ­
            with gr.Tab("Vote"):
                gr.Markdown("# 2025 Winter P-Tech Team1 LLM Test Page \n"
                            "## Guide-line \n"
                            "### - ì¶”ê°€ í•  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìž…ë ¥í•˜ì„¸ìš”. ê·¸ëƒ¥ ì§„í–‰ í•˜ì…”ë„ ë©ë‹ˆë‹¤.\n"
                            "### - ì§ˆë¬¸ì„ ë³´ë‚¸ í›„ ë‹µë³€ì„ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.\n"
                            "### - ì—…/ë‹¤ìš´ ìƒíƒœ ë³€í™˜ì„ í†µí•˜ì—¬ ìžìœ ë¡­ê²Œ í‰ê°€ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n"
                            "### - ìµœì¢… ì„ íƒ ë²„íŠ¼ì„ í´ë¦­ ì‹œ ì œì¶œë©ë‹ˆë‹¤.")
                with gr.Row():
                    user_question = gr.Textbox(label="ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”", lines=1)
                    systemp = gr.Textbox(label="ì¶”ê°€í•  ì‹œìŠ¤í…œ í”„ë¡œì íŠ¸ë¥¼ ìž…ë ¥í•˜ì„¸ìš”", lines=1)
                submit_btn = gr.Button("ì§ˆë¬¸ ë³´ë‚´ê¸°")

                with gr.Row():
                    with gr.Column(elem_id="colA") as colA:
                        respA = gr.Textbox(label="Model_A ì‘ë‹µ", lines=1, interactive=False)
                        toggleA = gr.Button("Model_A (ðŸ‘Ž)")
                    with gr.Column(elem_id="colB") as colB:
                        respB = gr.Textbox(label="Model_B ì‘ë‹µ", lines=1, interactive=False)
                        toggleB = gr.Button("Model_B (ðŸ‘Ž)")
                    with gr.Column(elem_id="colC") as colC:
                        respC = gr.Textbox(label="Model_C ì‘ë‹µ", lines=1, interactive=False)
                        toggleC = gr.Button("Model_C (ðŸ‘Ž)")
                    with gr.Column(elem_id="colD") as colD:
                        respD = gr.Textbox(label="Model_D ì‘ë‹µ", lines=1, interactive=False)
                        toggleD = gr.Button("Model_D (ðŸ‘Ž)")
                
                round_msg = gr.Textbox(label="ë¼ìš´ë“œ ì•ˆë‚´", lines=2, interactive=False)
                round_btn = gr.Button("ë¼ìš´ë“œ í•œë²ˆ ë” ì§„í–‰")

                final_msg = gr.Textbox(label="ìµœì¢… ê²°ê³¼ ì•ˆë‚´", lines=2, interactive=False)
                final_btn = gr.Button("ìµœì¢… ì„ íƒ")

                restart_btn = gr.Button("ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œìž‘", visible=False)

            # Leaderboard íƒ­
            with gr.Tab("Leaderboard"):
                gr.Markdown("## ë¦¬ë”ë³´ë“œ í™”ë©´ (Scoreboard)")
                # States
                init_models = ["Model_A","Model_B","Model_C","Model_D"]
                active_models_state = gr.State(init_models)
                vote_state = gr.State({m:"down" for m in init_models})
                final_series_state = gr.State(pd.Series([], dtype=object))
                auto_finalized_state = gr.State(False)

                score_state = gr.State({m:0 for m in init_models})  # ì ìˆ˜
                scoreboard_df = gr.Dataframe(
                headers=["Model","Score"],
                datatype=["str","number"],
                value=[],
                label="ë¦¬ë”ë³´ë“œ",
                interactive=False
              )
                scoreboard_df
                gr.Markdown("ëª¨ë¸ë³„ ì ìˆ˜ë¥¼ ì—¬ê¸°ì„œ í™•ì¸í•˜ì„¸ìš”. íˆ¬í‘œëŠ” Vote íƒ­ì—ì„œ ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

        # (1) ì§ˆë¬¸ ë³´ë‚´ê¸°
        submit_btn.click(
            fn=submit_question,
            inputs=[systemp, user_question, active_models_state, vote_state],
            outputs=[respA, respB, respC, respD, vote_state]
        )

        # (2) ì—…/ë‹¤ìš´ í† ê¸€
        toggleA.click(fn=lambda vs:toggle_vote(vs,"Model_A"), inputs=[vote_state], outputs=[vote_state, toggleA])
        toggleB.click(fn=lambda vs:toggle_vote(vs,"Model_B"), inputs=[vote_state], outputs=[vote_state, toggleB])
        toggleC.click(fn=lambda vs:toggle_vote(vs,"Model_C"), inputs=[vote_state], outputs=[vote_state, toggleC])
        toggleD.click(fn=lambda vs:toggle_vote(vs,"Model_D"), inputs=[vote_state], outputs=[vote_state, toggleD])

        # (3) ë¼ìš´ë“œ ì§„í–‰
        round_btn.click(
            fn=next_round_and_auto_finalize,
            inputs=[vote_state, active_models_state],
            outputs=[
                round_msg,
                active_models_state,
                colA, colB, colC, colD,
                final_msg,
                final_series_state,
                auto_finalized_state,
                restart_btn
            ]
        )

        # (4) ìµœì¢… ì„ íƒ -> ì ìˆ˜ ê°±ì‹ 
        def finalize_wrapper(vs, am, sc):
            msg, fseries, af, rst_btn, new_sc, new_df = finalize_models_score(vs, am, sc)
            return msg, fseries, af, rst_btn, new_sc, new_df

        final_btn.click(
            fn=finalize_wrapper,
            inputs=[vote_state, active_models_state, score_state],
            outputs=[final_msg, final_series_state, auto_finalized_state, restart_btn, score_state, scoreboard_df]
        )

        # (5) ì²˜ìŒë¶€í„° -> íˆ¬í‘œë§Œ ë¦¬ì…‹
        def restart_wrapper(am, vs, fs):
            return restart_all_but_keep_score(am, vs, fs)

        restart_btn.click(
            fn=restart_wrapper,
            inputs=[active_models_state, vote_state, final_series_state],
            outputs=[
                active_models_state, vote_state, final_series_state,
                colA, colB, colC, colD,
                round_msg, final_msg,
                auto_finalized_state,
                restart_btn
            ]
        )

    return demo

if __name__=="__main__":
    app = build_app()
    app.launch(share=True)
    