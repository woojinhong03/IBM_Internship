# -*- coding: utf-8 -*-
import gradio as gr
import pandas as pd
from dotenv import load_dotenv
import os


####################################
# IBM_Cloud Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞
####################################

from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods
from ibm_watsonx_ai import Credentials
from ibm_watsonx_ai.foundation_models import ModelInference

Watsonx_ai_url = 'https://us-south.ml.cloud.ibm.com'

load_dotenv()
IBM_Cloud_API = os.getenv('api_key')
IBM_Project_ID = os.getenv('project_id')

def IBM_LLMS(ibm_model,systemp,DATA):
	# ibm_model = 'ibm/granite-3-8b-instruct'
	ibm_models=['ibm/granite-3-8b-instruct','ibm/granite-3-2-8b-instruct-preview-rc','meta-llama/llama-3-1-8b-instruct']
	Pindex=0 if ibm_models.index(ibm_model)<=1 else ibm_models.index(ibm_model)-1
	credentials = Credentials(
    	url=Watsonx_ai_url,
    	api_key=IBM_Cloud_API,
	)
	parameters = {
		GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,
    	GenParams.MAX_NEW_TOKENS: 1000,
    	GenParams.MIN_NEW_TOKENS: 1,
    	GenParams.TEMPERATURE: 0.5,
    	GenParams.TOP_K: 50,
    	GenParams.TOP_P: 1
	}
	model = ModelInference(
    	model_id = ibm_model,
    	params = parameters,
    	credentials = credentials,
    	project_id = IBM_Project_ID,
    )
 
# Í∏∞Î≥∏ ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ Ï†ïÏùò
	prompt_inputs = [f"""
<|start_of_role|>system<|end_of_role|>
Knowledge Cutoff Date: April 2024.
Today's Date: December 16, 2024.
You are Granite, developed by IBM. You are a helpful AI assistant with access to the following tools. When a tool is required to answer the user's query, respond with <|tool_call|> followed by a JSON list of tools used. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request.
Answer questions briefly and do not provide unnecessary explanation.
You are a Korean model, SO ANSWER IN KOREAN.{systemp}.Don't fill in the format.
User question:{DATA}<|end_of_text|>""",
f"""<|start_header_id|>system<|end_header_id|>
You always answer the questions with markdown formatting using GitHub syntax. 
The markdown formatting you support: headings, bold, italic, links, tables, lists, code blocks, and blockquotes. 
You must omit that you answer the questions with markdown.
Any HTML tags must be wrapped in block quotes, for example ```<html>```. 
You will be penalized for not rendering code in block quotes.
When returning code blocks, specify language.
You are a helpful, respectful and honest assistant. 
Always answer as helpfully as possible, while being safe.
Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. 
Please ensure that your responses are socially unbiased and positive in nature.
If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. 
If you don't know the answer to a question, please don't share false information.<|eot_id|><|start_header_id|>user<|end_header_id|>
Only answer questions concisely.
You are a Korean model, SO ANSWER IN KOREAN.{systemp}.Don't fill in the format.
User question:{DATA}<|end_of_text|>"""]

	generated_response = model.generate_text(prompt=prompt_inputs[Pindex], guardrails=False)
 
	return (f"{generated_response}")


####################################
# Google_Gemini Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞
####################################

import google.generativeai as genai
Google_Gemini_API = os.getenv('google_api_key')

def GOOGLE_LLM(systemp, DATA):
	SYSTEM_PROMPT = systemp
	genai.configure(api_key=Google_Gemini_API)
	model = genai.GenerativeModel('gemini-1.5-flash')
	response = model.generate_content(SYSTEM_PROMPT + DATA)
	res = {response.text}

	return list(res)[0].strip('\n?')


####################################
# ÏßàÎ¨∏ Î≥¥ÎÇ¥Í∏∞
####################################

def submit_question(systemp, question, active_models, vote_state):
    if not question.strip():
        return ("ÏßàÎ¨∏Ïù¥ ÎπÑÏñ¥ÏûàÏäµÎãàÎã§.",)*4 + (vote_state,)

    responses = simulate_model_responses(systemp, question, active_models)
    rA = responses.get("Model_A", "Model_AÎäî Ï†úÏô∏Îê®.")
    rB = responses.get("Model_B", "Model_BÎäî Ï†úÏô∏Îê®.")
    rC = responses.get("Model_C", "Model_CÎäî Ï†úÏô∏Îê®.")
    rD = responses.get("Model_D", "Model_DÎäî Ï†úÏô∏Îê®.")

    return rA, rB, rC, rD, vote_state


####################################
# Í∞ÄÏÉÅ Î™®Îç∏ ÏùëÎãµ
####################################

def simulate_model_responses(systemp, question, model_list):
    """
    Ïã§Ï†ú Î™®Îç∏ Ìò∏Ï∂ú ÎåÄÏã† Í∞ÑÎã®Ìûà Í∞ÄÏÉÅÏùò ÏùëÎãµÏùÑ ÎßåÎì≠ÎãàÎã§.
    """
    responses = {}
    real_model = ['gemini-1.5-flash', 'ibm/granite-3-8b-instruct','ibm/granite-3-2-8b-instruct-preview-rc','meta-llama/llama-3-1-8b-instruct']

    cnt = 0
    for m in model_list:
        if cnt == 0:
            responses[m] = GOOGLE_LLM(systemp, question)
        else:
            responses[m] = IBM_LLMS(real_model[cnt], systemp, question)
        cnt += 1
    
    return responses


####################################
# ÎùºÏö¥Îìú ÏßÑÌñâ (ÏûêÎèô ÌôïÏ†ï)
####################################

def next_round_and_auto_finalize(vote_state, active_models):
    model_match = {"Model_A":'gemini-1.5-flash', "Model_B":'ibm/granite-3-8b-instruct',"Model_C":'ibm/granite-3-2-8b-instruct-preview-rc',"Model_D":'meta-llama/llama-3-1-8b-instruct'}
    up_models = [m for m in active_models if vote_state.get(m, "‚ùå") == "‚≠ï"]
    auto_final = False
    final_msg = ""
    final_series = pd.Series([], dtype=object)
    
    if len(up_models) == 0:
        round_msg = "ÌòÑÏû¨ ÎùºÏö¥ÎìúÏóêÏÑú '‚≠ï'Îêú Î™®Îç∏Ïù¥ ÏóÜÏäµÎãàÎã§. Î™®Îëê ÌÉàÎùΩ."
        new_models = []
    elif len(up_models) == 1:
        only_m = up_models[0]
        round_msg = f"'{only_m}' Ìïú Í∞úÎßå '‚≠ï' => ÏûêÎèô ÏµúÏ¢Ö ÌôïÏ†ï!"
        final_msg = f"ÏµúÏ¢Ö Î™®Îç∏ÏùÄ '{model_match[only_m]}'ÏûÖÎãàÎã§!"
        final_series = pd.Series([only_m])
        auto_final = True
        new_models = [only_m]
    else:
        round_msg = f"'‚≠ï'Îêú Î™®Îç∏: {up_models}"
        new_models = up_models

    hideA = gr.update(visible=("Model_A" in new_models))
    hideB = gr.update(visible=("Model_B" in new_models))
    hideC = gr.update(visible=("Model_C" in new_models))
    hideD = gr.update(visible=("Model_D" in new_models))
    
    show_restart = gr.update(visible=auto_final)

    return (
        round_msg,
        new_models,
        hideA, hideB, hideC, hideD,
        final_msg,
        final_series,
        auto_final,
        show_restart
    )
    

####################################
# ÏóÖ/Îã§Ïö¥ ÌÜ†Í∏Ä
####################################

def toggle_vote(vote_state, model):
    current = vote_state.get(model, "‚ùå")  # Í∏∞Î≥∏Í∞íÏùÑ "X"Î°ú ÏÑ§Ï†ï
    new_val = "‚≠ï" if current == "‚ùå" else "‚ùå"  # O/XÎ°ú ÌÜ†Í∏Ä
    vote_state[model] = new_val

    emoji = "‚≠ï" if new_val == "‚≠ï" else "‚ùå"  # O/X Ïù¥Î™®ÏßÄÎ°ú Î≥ÄÍ≤Ω
    label = f"{model} ({emoji})"
    
    return vote_state, label


####################################
# Î¶¨ÎçîÎ≥¥Îìú Í∞±Ïã†
####################################

def update_score(score_dict, final_series):
    """
    final_series ÎÇ¥ Î™®Îç∏Îì§ => +1Ï†ê
    """
    if not final_series.empty:
        for m in final_series:
            score_dict[m] = score_dict.get(m, 0) + 1

    df = pd.DataFrame(list(score_dict.items()), columns=["Model","Score"])
    df.sort_values(by="Score", ascending=False, inplace=True)
    return score_dict, df

def finalize_models_score(vote_state, active_models, score_dict):
    model_match = {"Model_A":'gemini-1.5-flash', "Model_B":'ibm/granite-3-8b-instruct',"Model_C":'ibm/granite-3-2-8b-instruct-preview-rc',"Model_D":'meta-llama/llama-3-1-8b-instruct'}
    """
    ÏóÖÎêú Î™®Îç∏Îì§ => ÏµúÏ¢Ö ÌôïÏ†ï => Ï†êÏàò Î∞òÏòÅ
    """
    ups = [m for m in active_models if vote_state.get(m, "‚ùå") == "‚≠ï"]
    final_series = pd.Series(ups, dtype=object)

    if len(ups) == 0:
        msg = "ÏóÖÎêú Î™®Îç∏Ïù¥ ÏóÜÏäµÎãàÎã§. ÏµúÏ¢ÖÏÑ†ÌÉù Î∂àÍ∞Ä."
    elif len(ups) == 1:
        msg = f"ÏµúÏ¢Ö Î™®Îç∏ÏùÄ '{model_match[ups[0]]}'ÏûÖÎãàÎã§!"
    else:
        msg = f"ÏµúÏ¢Ö Î™®Îç∏Ïù¥ Ïó¨Îü¨ Í∞úÏûÖÎãàÎã§: {ups}"

    auto_final = (len(ups) >= 1)
    show_restart = gr.update(visible=auto_final)

    # Ï†êÏàò ÏóÖÎç∞Ïù¥Ìä∏
    new_score, new_df = update_score(score_dict, final_series)

    return msg, final_series, auto_final, show_restart, new_score, new_df


####################################
# Ï≤òÏùåÎ∂ÄÌÑ∞ Îã§Ïãú ÏãúÏûë (Ï†êÏàò Ïú†ÏßÄ)
####################################

def restart_all_but_keep_score(active_models, vote_state, final_series):
    """Ìà¨Ìëú ÏÉÅÌÉúÎßå Ï¥àÍ∏∞Ìôî, Ï†êÏàò Í∑∏ÎåÄÎ°ú"""
    init_models = ["Model_A","Model_B","Model_C","Model_D"]
    new_vote = {m:"down" for m in init_models}
    new_series = pd.Series([], dtype=object)

    # Î™®Îç∏ Ïó¥ Î™®Îëê visible=True Î≥µÍµ¨
    showA= gr.update(visible=True)
    showB= gr.update(visible=True)
    showC= gr.update(visible=True)
    showD= gr.update(visible=True)
    showE= gr.update(visible=True)
    showF= gr.update(visible=True)

    round_msg = "ÏÉàÎ°ú ÏãúÏûëÌï©ÎãàÎã§. ÏßàÎ¨∏ ÏûÖÎ†• ÌõÑ ÏßÑÌñâÌïòÏÑ∏Ïöî."
    final_msg = ""
    hide_restart = gr.update(visible=False)

    return (
        init_models, new_vote, new_series,    # active_models, vote_state, final_series
        showA, showB, showC, showD, showE, showF,
        round_msg, final_msg,
        False,     # auto_finalized
        hide_restart
    )


####################################
# Î©îÏù∏ App (Tabs: Vote / Leaderboard)
####################################

def build_app():
    with gr.Blocks(css="""
                   .gradio-container {
                        max-width: 1280px;
                        margin: auto;
                        h1{text-align: center;};
                        box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);
                        border-radius: 10px;
                        padding: 20px;
                    }""") as demo:
        with gr.Tabs():
            with gr.Tab("Main"):
                gr.Markdown("""
            # üèÜ LLM Î™®Îç∏ ÏÑ±Îä• ÎπÑÍµê ÌîÑÎ°úÏ†ùÌä∏
            
            Ïù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî **LLM Î™®Îç∏ Í∞ÑÏùò ÏÑ±Îä•ÏùÑ ÎπÑÍµê**ÌïòÍ≥†, 
            ÌäπÌûà **ÌïúÍµ≠Ïñ¥ ÏÇ¨Ïö©Ïùò ÏõêÌôúÌï®**ÏùÑ Î∂ÑÏÑùÌïòÎäî Í≤ÉÏùÑ Î™©ÌëúÎ°ú Ìï©ÎãàÎã§.
                        
            ÏãúÏ§ëÏóêÎäî Ïù¥ÎØ∏ Îã§ÏñëÌïú Ï†ïÎüâÏ†Å ÏßÄÌëúÍ∞Ä Ï°¥Ïû¨ÌïòÏßÄÎßå, Ïã§Ï†úÎ°ú Ïö∞Î¶¨Í∞Ä ÏßÅÏ†ë ÏÇ¨Ïö©Ìï¥Î≥∏ Í≤∞Í≥º, Ïù¥Îü¨Ìïú ÏßÄÌëúÏùò ÏàúÏúÑÏôÄÎäî Îã§Î•∏ ÏÑ±Îä•ÏùÑ Î≥¥Ïù¥Îäî Í≤ΩÏö∞Í∞Ä ÎßéÏïòÏäµÎãàÎã§.  
            Ïù¥Ïóê Îî∞Îùº, Îã®ÏàúÌûà Ï†êÏàòÎ°ú ÎÇòÌÉÄÎÇú ÏßÄÌëúÎßåÏù¥ ÏïÑÎãàÎùº, Ïã§Ï†ú ÏÇ¨Ïö©ÏûêÎì§Ïù¥ Ï≤¥Í∞êÌïòÎäî ÎßåÏ°±ÎèÑÎ•º Í∏∞Î∞òÏúºÎ°ú ÏµúÏ†ÅÏùò Î™®Îç∏ÏùÑ ÏÑ†Ï†ïÌïòÎäî Îç∞ ÎèÑÏõÄÏùÑ Ï£ºÍ≥†Ïûê ÌïòÏòÄÏäµÎãàÎã§.  
            ÎòêÌïú, Ïö∞Î¶¨Í∞Ä ÏßÅÏ†ë ÌèâÍ∞ÄÌïú Í≤∞Í≥ºÎ•º Ï∞∏Í≥†ÌïòÎäî Í≤ÉÎøêÎßå ÏïÑÎãàÎùº, ÏÇ¨Ïö©ÏûêÍ∞Ä ÏßÅÏ†ë Î™®Îç∏ÏùÑ Ï≤¥ÌóòÌïòÍ≥† ÌèâÍ∞ÄÌï®ÏúºÎ°úÏç® ÏûêÏã†ÏóêÍ≤å Í∞ÄÏû• Ï†ÅÌï©Ìïú Î™®Îç∏ÏùÑ Ï∞æÏùÑ Ïàò ÏûàÎèÑÎ°ù ÎèïÎäî Í≤ÉÏù¥ Î™©ÌëúÏûÖÎãàÎã§.
            
            üöÄ **Î™®Îç∏ ÏÑ†Ï†ï Í∏∞Ï§Ä**
            - ÌååÎùºÎØ∏ÌÑ∞(Parameter) Í∞úÏàò
            - AI Î™®Îç∏ Î≤ÑÏ†Ñ
            - ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïñ∏Ïñ¥
            - Î¶¨ÏÜåÏä§ ÏÇ¨Ïö©Îüâ
            
            üìå **Î™®Îç∏ ÌèâÍ∞Ä Î∞©Ïãù**
            - ÏÇ¨ÎûåÏù¥ ÏßÅÏ†ë ÌèâÍ∞ÄÌïòÎäî **Human Evaluation** Î∞©Ïãù
            - ÎèôÏùºÌïú ÏßÄÎ¨∏Í≥º ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏó¨ ÎπÑÍµê
            - ÏûÖÎ†•Îêú Îç∞Ïù¥ÌÑ∞ ÌÜµÏùºÌïòÏó¨ ÏùºÍ¥ÄÏÑ± Ïú†ÏßÄ
            
            ‚úÖ **Î™®Îç∏ ÌèâÍ∞Ä Í∏∞Ï§Ä**
            - Ï†ïÎ≥¥ Ï†ïÌôïÎèÑ
            - Î¨∏Ïû• ÏôÑÏÑ±ÎèÑ
            - Î¨∏Ìï¥Î†•
            - ÎÖºÎ¶¨Ï†Å Í∑ºÍ±∞
            - ÏïàÏ†ÑÏÑ± Î∞è Ìé∏Ìñ•ÏÑ± 
        """)

# ÏÇ¨Ïö© Î™®Îç∏ Ï†ïÎ≥¥(key model granite 3v)
                gr.Markdown("### üìã LLM Î™®Îç∏ ÏÑ±Îä• ÎπÑÍµê ÌÖåÏù¥Î∏î")

                scoreboard_df1 = gr.Dataframe(
                headers=["Î™®Îç∏Î™Ö", "ÌååÎùºÎØ∏ÌÑ∞ Í∞úÏàò", "Ïª®ÌÖçÏä§Ìä∏ ÌÅ¨Í∏∞", "ÏûÑÎ≤†Îî© ÌÅ¨Í∏∞"],
                datatype=["str","str",'str','str'],
                value=[["Granite-3-8B-Instruct", "8B", "128,000 tokens", "4096"],
            ["Granite-3.2-8B-Instruct-Preview", "8B", "128,000 tokens", "4096"],
            ["Meta-Llama-3-8B", "8B", "128,000 tokens", "4096"],
            ["Gemini 1.5 Flash-8B", "8B", "1,048,576 tokens", "2048"]],
                label="Î¶¨ÎçîÎ≥¥Îìú",
                interactive=False
              )
                scoreboard_df1

                gr.Image("image.png", label="üìä LLM Î™®Îç∏ ÎπÑÍµê Î∂ÑÏÑù")
            # Vote ÌÉ≠
            with gr.Tab("Vote"):
                gr.Markdown("# 2025 Winter P-Tech Team1 LLM Test Page \n"
                            "## Guide-line \n"
                            "### - Ï∂îÍ∞Ä Ìï† ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî. Í∑∏ÎÉ• ÏßÑÌñâ ÌïòÏÖîÎèÑ Îê©ÎãàÎã§.\n"
                            "### - ÏßàÎ¨∏ÏùÑ Î≥¥ÎÇ∏ ÌõÑ ÎãµÎ≥ÄÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÌèâÍ∞ÄÌïòÏÑ∏Ïöî.\n"
                            "### - ÏóÖ/Îã§Ïö¥ ÏÉÅÌÉú Î≥ÄÌôòÏùÑ ÌÜµÌïòÏó¨ ÏûêÏú†Î°≠Í≤å ÌèâÍ∞Ä Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n"
                            "### - ÏµúÏ¢Ö ÏÑ†ÌÉù Î≤ÑÌäºÏùÑ ÌÅ¥Î¶≠ Ïãú Ï†úÏ∂úÎê©ÎãàÎã§.\n\n\n"
                            "## Î™®Îç∏ ÌÖåÏä§Ìä∏")
                with gr.Row():
                    user_question = gr.Textbox(label="ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî", lines=1)
                    systemp = gr.Textbox(label="Ï∂îÍ∞ÄÌï† ÏãúÏä§ÌÖú ÌîÑÎ°úÏ†ùÌä∏Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî", lines=1)
                submit_btn = gr.Button("ÏßàÎ¨∏ Î≥¥ÎÇ¥Í∏∞")

                with gr.Row():
                    with gr.Column(elem_id="colA") as colA:
                        respA = gr.Textbox(label="Model_A ÏùëÎãµ", lines=1, interactive=False)
                        toggleA = gr.Button("Model_A (‚ùå)")
                    with gr.Column(elem_id="colB") as colB:
                        respB = gr.Textbox(label="Model_B ÏùëÎãµ", lines=1, interactive=False)
                        toggleB = gr.Button("Model_B (‚ùå)")
                    with gr.Column(elem_id="colC") as colC:
                        respC = gr.Textbox(label="Model_C ÏùëÎãµ", lines=1, interactive=False)
                        toggleC = gr.Button("Model_C (‚ùå)")
                    with gr.Column(elem_id="colD") as colD:
                        respD = gr.Textbox(label="Model_D ÏùëÎãµ", lines=1, interactive=False)
                        toggleD = gr.Button("Model_D (‚ùå)")
                
                round_msg = gr.Textbox(label="ÎùºÏö¥Îìú ÏïàÎÇ¥", lines=2, interactive=False)
                round_btn = gr.Button("ÎùºÏö¥Îìú ÌïúÎ≤à Îçî ÏßÑÌñâ")

                final_msg = gr.Textbox(label="ÏµúÏ¢Ö Í≤∞Í≥º ÏïàÎÇ¥", lines=2, interactive=False)
                final_btn = gr.Button("ÏµúÏ¢Ö ÏÑ†ÌÉù")

                restart_btn = gr.Button("Ï≤òÏùåÎ∂ÄÌÑ∞ Îã§Ïãú ÏãúÏûë", visible=False)

            # Leaderboard ÌÉ≠
            
            # import page2
            
            with gr.Tab("Your Leaderboard"):
                gr.Markdown("## ÎÇòÎßåÏùò Î¶¨ÎçîÎ≥¥Îìú ÌôîÎ©¥ (Scoreboard)")
                # States
                init_models = ["Model_A","Model_B","Model_C","Model_D"]
                active_models_state = gr.State(init_models)
                vote_state = gr.State({m: "‚ùå" for m in init_models})  # Í∏∞Î≥∏Í∞í "X"
                final_series_state = gr.State(pd.Series([], dtype=object))
                auto_finalized_state = gr.State(False)

                score_state = gr.State({m:0 for m in init_models})  # Ï†êÏàò
                scoreboard_df = gr.Dataframe(
                headers=["Model","Score"],
                datatype=["str","number"],
                value=[],
                label="Î¶¨ÎçîÎ≥¥Îìú",
                interactive=False
              )
                scoreboard_df
                gr.Markdown("ÎãπÏã†Ïù¥ ÌÖåÏä§Ìä∏Ìïú Ï†êÏàòÎ•º Ïó¨Í∏∞ÏÑú ÌôïÏù∏ÌïòÏÑ∏Ïöî. Ìà¨ÌëúÎäî Vote ÌÉ≠ÏóêÏÑú ÏßÑÌñâ Í∞ÄÎä•Ìï©ÎãàÎã§.")

        # (1) ÏßàÎ¨∏ Î≥¥ÎÇ¥Í∏∞
        submit_btn.click(
            fn=submit_question,
            inputs=[systemp, user_question, active_models_state, vote_state],
            outputs=[respA, respB, respC, respD, vote_state]
        )

        # (2) ÏóÖ/Îã§Ïö¥ ÌÜ†Í∏Ä
        toggleA.click(fn=lambda vs: toggle_vote(vs, "Model_A"), inputs=[vote_state], outputs=[vote_state, toggleA])
        toggleB.click(fn=lambda vs: toggle_vote(vs, "Model_B"), inputs=[vote_state], outputs=[vote_state, toggleB])
        toggleC.click(fn=lambda vs: toggle_vote(vs, "Model_C"), inputs=[vote_state], outputs=[vote_state, toggleC])
        toggleD.click(fn=lambda vs: toggle_vote(vs, "Model_D"), inputs=[vote_state], outputs=[vote_state, toggleD])
        
        
        # (3) ÎùºÏö¥Îìú ÏßÑÌñâ
        round_btn.click(
            fn=next_round_and_auto_finalize,
            inputs=[vote_state, active_models_state],
            outputs=[
                round_msg,
                active_models_state,
                colA, colB, colC, colD,
                final_msg,
                final_series_state,
                auto_finalized_state,
                restart_btn
            ]
        )

        # (4) ÏµúÏ¢Ö ÏÑ†ÌÉù -> Ï†êÏàò Í∞±Ïã†
        def finalize_wrapper(vs, am, sc):
            msg, fseries, af, rst_btn, new_sc, new_df = finalize_models_score(vs, am, sc)
            return msg, fseries, af, rst_btn, new_sc, new_df

        final_btn.click(
            fn=finalize_wrapper,
            inputs=[vote_state, active_models_state, score_state],
            outputs=[final_msg, final_series_state, auto_finalized_state, restart_btn, score_state, scoreboard_df]
        )

        # (5) Ï≤òÏùåÎ∂ÄÌÑ∞ -> Ìà¨ÌëúÎßå Î¶¨ÏÖã
        def restart_wrapper(am, vs, fs):
            return restart_all_but_keep_score(am, vs, fs)

        restart_btn.click(
            fn=restart_wrapper,
            inputs=[active_models_state, vote_state, final_series_state],
            outputs=[
                active_models_state, vote_state, final_series_state,
                colA, colB, colC, colD,
                round_msg, final_msg,
                auto_finalized_state,
                restart_btn
            ]
        )

    return demo

if __name__=="__main__":
    app = build_app()
    app.launch(share=True)
    